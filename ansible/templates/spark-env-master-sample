##################################################################################################################
# Master node requires a lot of memory.  We allocate 64G for the driver to avoid OOM when
# there are a lot of tasks.
# 
# EXECUTOR_MEMORY and EXECUTOR_CORES are used in the spark-submit job and controls the memory
# and CPUs per executor. Here are our recommendations to set the number of cores per executor:
#  - it should be between 5 and 10
#  - the total number of cores per worker node should be a multiple of the number of cores
#    per executor
#  - it can not be higher than the number of cores on a worker node
# For example, if there are 16 cores on each worker node, we could set EXECUTOR_CORES=8,
# or if there are 2 cores on each worker node, we could set EXECUTOR_CORES=2.
#
# By using multiple worker nodes, we can control the velocity of the migration.
#
# Eg. 
#    Target system is 3 x i4i.4xlarge (16 vCPU, 128G)
#    We can provision 2 x m7g.2xlarge (8vCPU, 32G)
#
#    EXECUTOR_CORES=8
#    EXECUTOR_MEMORY=16G    # (8 cores per worker * 2G)
#
#    Start the Spark master node and the Spark worker node, and run the migration.
#    - Monitor the pressure on the source system
#    - Monitor the velocity of the migration in the spark jobs monitoring UI
#    - If necessary, provision more worker nodes
##################################################################################################################

export SPARK_MASTER_HOST={{ hostvars.spark_master.ansible_default_ipv4.address }}
export EXECUTOR_CORES=4
# By default, allocate 2GB of memory per core
export EXECUTOR_MEMORY="$((EXECUTOR_CORES * 2))G"
